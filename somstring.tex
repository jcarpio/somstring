%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%%% This is a general template file for the LaTeX package SVJour3% for Springer journals.          Springer Heidelberg 2006/03/15%% Copy it to a new file with a new name and use it as the basis% for your article. Delete % signs as needed.%% This template includes a few options for different layouts and% content for various journals. Please consult a previous issue of% your journal as needed.%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% First comes an example EPS file -- just ignore it and% proceed on the \documentclass line% your LaTeX will extract the file if required\begin{filecontents*}{example.eps}%!PS-Adobe-3.0 EPSF-3.0%%BoundingBox: 19 19 221 221%%CreationDate: Mon Sep 29 1997%%Creator: programmed by hand (JK)%%EndCommentsgsavenewpath  20 20 moveto  20 220 lineto  220 220 lineto  220 20 linetoclosepath2 setlinewidthgsave  .4 setgray fillgrestorestrokegrestore\end{filecontents*}%\documentclass{svjour3}                     % onecolumn (standard format)%\documentclass[smallextended]{svjour3}     % onecolumn (second format)%\documentclass[twocolumn]{svjour3}         % twocolumn%\smartqed  % flush right qed marks, e.g. at end of proof%\usepackage{graphicx}\usepackage{amssymb, amsmath} % AÃ±adido por JCC 5/04/2010%% \usepackage{mathptmx}      % use Times fonts if available on your TeX system%% insert here the call for the packages your document requires%\usepackage{latexsym}% etc.%% please place your own definitions here and don't use \def but% \newcommand{}{}%% Insert the name of "your journal" with% \journalname{myjournal}%\begin{document}\title{Nonvectorial data clasification with Self-organizing maps using data interpolation v.29%\thanks{Grants or other notes%about the article that should go on the front page should be%placed here. General acknowledgments should be placed at the end of the article.}}% \subtitle{Do you have a subtitle?\\ If so, write it here}% \titlerunning{Short form of title}        % if too long for running head\author{J. Carpio         \and        J.J. Merelo \and V. Rivas %etc.}%\authorrunning{Short form of author list} % if too long for running head\institute{J. Carpio \at              Information Technology Department\\              University of Huelva \\              Tel.: +34-959-217658\\              Fax: +34-959-217658\\              \email{jose.carpio@dti.uhu.es}           %  \\%             \emph{Present address:} of F. Author  %  if needed           \and           J. Merelo \at              Computers Architecture and Technology Department\\              University of Granada\\              Tel.: +34-958-243162\\              Fax: +34-958-243162\\              \email{jmerelo@geneura.ugr.es}           %  \\%             \emph{Present address:} of F. Author  %  if needed           \and           V. Rivas \at              Computer Science Department\\              University of Jaen\\              Tel.: +34-953-243162\\              Fax: +34-953-243162\\              \email{vrivas@ujaen.es}           %  \\%             \emph{Present address:} of F. Author  %  if needed}\date{Received: date / Accepted: date}% The correct dates will be entered by the editor\maketitle\begin{abstract}Self-organizing maps (SOM) are widely used to classify vectorial data. However, its methodology is not restricted to metric vector spaces. In this paper we introduces a self-organized map framework to classify any kind of data with only two restrictions: namely, the existence of a distance measure for input elements, and the possibility to interpolate a new element between other two with a certain distance. We formilizes the requiriments to define a distance measure and the interpolation functions and, as a practical example, following this philosophy we implemented a SOM for symbol strings.\keywords{Self-organizing maps; Framework; Clustering; Visualization; Data mining; Nonvectorial data}% \PACS{PACS code1 \and PACS code2 \and more}% \subclass{MSC code1 \and MSC code2 \and more}\end{abstract}%---------------------------------------------------------------------%  Introduction %---------------------------------------------------------------------\section{Introduction}\label{intro}\cite{Hammer05classificationusing} The key ingredient of pupular machine learning methods such as suppor vector machines (SVM), learning vector quantization (LVQ),self-organizing maps (SOM), or K-nearest neighbor classification is a similarity measure of the data in the input space....The choice of the metric for these methods is directly connected to the representation of data and it crucially influences theefficiency, accuracy, and generalization ability of the results. Depending on the respective application, different aims need tobe fulfilled: the similarity measure should possess the flexibility to capture complexity inherent in the learning task; a powerfullmetric allows for sparse models and natural representation of date since complex decision borders \cite{hammer04a_bibuniq_253} A general framework for unsupervised processing of structured data: Unsupervised learning constitutes an alternative important paradigm for neural networks which has been successfully applied to data mining and visualization (see \cite{kohonen95a}).Up to now, only few approaches enlarge SOM to sequences: the temporal Kohonen map (TKM)\cite{chappell93a} and therecursive SOM (RSOM) \cite{voegtlin00a} and variations thereof. We are aware of only one approach which processesstree structured data and thus alse sequences in an unsupervised way: the SOM for structured data(SOMSD) \cite{sperduti01neural} .These models are trained by Hebbian learning. They have been applied to monitor standardtime series or image data, so far. Since a clear measure of the success of unsupervised processingof structured data and a theoretical formulation of the above dynamics and training method are lacking...\textbf{SOMSD}: Each neuron in the map is equiped with weights $(w, r_1, r_2)$. Hereby, $w \in \mathcal{W}$ iscontained in the set of possible labels, $r_1$ and $r_2$ are contained in $\mathbb{R}^d$, $d$ denotingthe dimensionality of the lattice. They correspond to possible indices of neurons. Given a tree $a(t_1, t_2)$as input, its distance from neuron $n$ with weights $(w, r_1, r_2)$ is recursively computed by $\alpha\parallel a-w\parallel+\beta\parallel\iota(t_1)-r_1\parallel+\beta\parallel\iota(t_2)-r_2\parallel$. Hereby, $\iota(t_1)$ and $\iota(t_2)$ denote the indices of the neurons with smallest distance from $t_1$ and$t_2$, respectively. \textbf{RSOM}: RSOM has originally been proposed for sequences in \cite{sperduti01neural} . However, it can be generalizedto tree structures. The neurons are equipped with weights $(w,r)$ where $w \in \mathcal{W}$. $r \in \mathbb{R}^N$ correspondsto the vector of activations of all neurons at the previous time step. \textbf{TKM}: The temporal Kohonen map consist of a lattecie of neurons where each neuron constitutes a leaky integrator \cite{chappell93a}.The approach is defined for sequences over the set $\mathcal{W}$, only. Each neuron is equiped with a weight $w \in \mathcal{W}$. The distance of the sequence $[a_0,...,a_{\iota}]$ from neuron $n$ weighted with $w$ is computed by ... \cite{lampinen89b} \begin{equation} \label{prediction} x(n+1) = X(n)^{T}W\end{equation} \begin{equation} \label{errors} e(n) = x(n+1) - X(n)^{T}W\end{equation} \begin{equation} \label{neighbors} W'= W + \gamma e(n)X(n)\end{equation}The self-organizing map of autoregressive (AR) models is a 2D lattice of units, indexed in the following by \textit{i},with weight vectors \textit{$W_{i}$} each signifying an AR process. According to Kohonen \cite{kohonen84a}, the general procedure in the lattice is:\begin{enumerate}    \item For each input sample, find the best matching unit    \item Modify that unit, and the units in its topological neigborhood, to further improve the match to the present input.   \end{enumerate}At a given step of the self-organizing algorithm, the inputs to the network, shared by all the units, are the samples in vectors $X(n)$.Each unit tries to predict $x(n+1)$ from Eq. (\ref{prediction}) by using its own weigth vector. Since the units are tuned to different ARmodels, some of the errors (\ref{errors}) will be smallest than the others. The apparent winner (the best matching unit) for each inputvector $X(n)$ is the unit with the smallest estimator error.Once the best-matching unit has been found, that unit and its neighbors are updated according to the \textit{alms} algorithm (\ref{neighbors}).To define the neigborhood in the 2D lattice, the distance \textit{r} of two units in the lattice must be defined. Neigborhoods determinedby both the Euclidean and the Chebyshev metrics have been tried and they give similar results. When the neigboring units are updated, the scalar adaptation $\gamma$ of Eq. (\ref{neighbors}) is multiplied by a factor depending on r to yield\begin{equation} \label{multiplied} g(r) = \gamma (1-\frac{r}{NE+1}),\end{equation}where $r$ is the distance from the best matching unit to the unit to be updated and parameter $N E$ determines the size of the neigborhood.The size is slowly decreasing in time, e.g., if $t$ denotes the iteration step, then\cite{kohonen98d} In the vast majority of SOM applications, the imput data constitute high-dimensional real \textit{feature vectors} $x \in \mathfrak{R}^{n} $,and the model vectors $m_{i} \in \mathfrak{R}^{n} $ are them approximations of the $x$ in somewhat similar sense as the codebook vectors inclassical vector quantization are. However, the models need not neccessarily be replica of the input vectors: the may be, e.g., parametric representations of operators that generate sequences of data \cite{lampinen89b} . On the other hand, there exist means to approximatealso non vectorial data, e.g., sets of symbol string can be approximated by ``average strings'' \cite{kohonen96l}. In theSOMs that forms similarity graphs of \textit{documents}, the models can still be taken as real vectors that describe collenctions of wordsin the documents. The models can simply be weighted histograms of the words, but usually some dimensionality reduction of the histogram is carried out, as we shall see next.\cite{kohonen98b} The self-organizing maps (SOMs) are usually defined in metric vector spaces. A different idea is organization of \textit{symbol strings} or other nonvectorial representation on a SOM array, whereby the relative locations of the images of the strings on a SOM array,whereby the relative locations of the images of the strings on the SOM ought to reflect, e.g., some distance measure, such as the\textit{Levenshtein distance} (LD) of \textit{feature distance} /FD) between strings (for textbook accounts, cf. ). If one triesto apply the SOM algorithm to such entities, the difficulty inmmediatly encountered is that \textit{incremental learning laws cannot beexpressed for symbols strings}, which are discrete entities. Neither can a string be regarded as a vector. On of the authors has shown that the SOM philosophy is nonetheless amenable to the construction of ordered similarity diagrams for string variables,if the following ideas are applied:\begin{enumerate} \item The \textit{batch map} principle \cite{kohonen95a} is used to define learning as a succession of \textit{conditional averages over subsets of selected strings}. \item The \'averages\' over the strings are computed as generalized \textit{means or mediand} ( reference to Kohonen median string) over the string.\end{enumerate}SOM for symbol strings has been analized in \cite{kohonen98b} using batch map principle \cite{kohonen95a}. The batch map principle is usedto define learning as a succession of condicional averages over subsets of selected strings and the averages over strings are computed as generalized means or medians over the strings.\cite{kohonen98b} -> Initialization of SOM for stringsIt is possible to initialize a usual vector-spaces SOM by radom vectorial values. We have been able to obtain organized SOMs for stringvariables, too, starting with random reference strings. However, it is a great advantage if the initial values are already ordered, evenroughly, along the SOM array. \cite{kohonen98b} -> The batch map for stringsThe conventional batch map computing steps \cite{kohonen95a} of the SOM are applicable to string variables almost as such:\begin{enumerate} \item For the initial reference strings, take, for instance, samples that are ordered two-dimensionally in the Sammon projection. \item For each map unit \textit{i}, collect a list of those sample strings to whoms the reference string of unit \textit{i} is the nearest reference string. \item For each map unit \textit{i}, take for the new reference string the mean or median over the union of the lists that belong to the topological neighborhood set \textit{$N_{i}$} of unit \textit{i}. \item Repeat from 2 a sufficient number of times, until the reference strings are no longer changed in further iteractions.\end{enumerate}\cite{kohonen02a_bibuniq_416} -> However, the SOM principle is not restricted to metric vector spaces. It has been pointed out \cite{kohonen96l} that any set of items, for wich a similarity or distance measure between its elements is defineable, can be mapped on the SOM grid in an ordely fashion. This is made possible by the following principle, which combines the concept of the \textit{generalized median} of a set \cite{ISI:A1985AVC8700004, kohonen95a} with the batch computation of the SOM. Assume a fundamental set $S$ of any kind of items $x(i)$ andlet $d[x(i),x(j)]$ be some distance measure between $x(i)$ and $x(j)$ and $x(j) \in S$. The generalized median $m$ over $S$ is definedas the item that minimizes the objetive function$D = {\displaystyle\sum_{x(i) \in S}} d[x(i),m]$In this work, $m$ is restricted to being an element of $S$. Notice that if the input samples had been real scalars and thedistance measure were the absolute value of their difference, it is easy to show that the generalized median coincides with the aritmetic median. On the other hand, if the input samples where real vectors, if the distance measure were Euclidean, and if thewith the smallest sum of the squares of distances from the other items where sought, the generalized median would coincide with the aritmetic mean of the $x(i) \in S$.PicSOM \cite{laaksonen00a} based on tree structured self-organizing maps (TS-SOMs). Given a set of reference images, PicSOM is ableto retrieve another set of images which are similar to the given ones. Each TS-SOM is formed with a differentimage feature representation like color, texture or shape. The queries are iteratively refined as the systemexposes images to the user. After the training phase, each unit of the TS-SOMs contains a model vector which may be regarded as the average of all featured vectors maped to a particular unit. A tree-structured, hierachical representationof all the images in the database is formed. In an ideal situation, there should be one-to-one correspondance betweenth images and TS-SOM units at the bottom level of each map. The five separate feature vectos obtained from each image in the database habe to be indexed to facilitatesimilarity-based queries. For the indexing, similarity-preserving clustering using SOMs (Kohonen book reference) is used.Due to the high dimensionality of the feature vectors and the large size of the image database, complexity is amajor issue.  The TS-SOM  \cite{koikkalainen90b, koikkalainen94b} is a tree-structured vector quantization algorithm thatuses SOMs at each of its hierarchical levels. PicSOM may use one or several types of statistical features for imagequerying. Separate feature vectors can be formed for describing the color, texture, shape, and structure of the images. A separate TS-SOM is then constructed for each feature. These maps are used in parallel to find from the databases those imageswhich are most similar to the images selected by the user. The feature selection is not restricted in any way, and newfeatures can be included as long as the feature vector are of fixed dimensionality and the Euclidean metric can be usedto measure distances between them.The continuous interpolating Self-organizig map (CI-SOM) \cite{ISI:A1997XM23700003} is based on I-SOM. Each neuron stores one corresponding codebook position in the input space and in the output space. The interpolating function passes exactly though these positions (support points). The dimensions of the interpolating function are equal to the dimensions of the map.\cite{Mora2007} ->  SOM makes a nonlinear projection from a high-dimensional data space (one dimension per variable) on a regular,low-dimensional data (usually 2D) grid of neurons, called units. SOM is further processed using ULtsch method (cite to Ultsch, S.: Kohonen's Self-organizing maps for exploratory data analysis), the Unified distance matrix (U-matrix) wich uses SOM's codevectors (vectors of variables of the problem) as data source and generates a matrix where each component is a distance measure between twoadjacent neurons. In order to apply SOM we need to transform this data set into another one where a distance between each pair of samples can be calculated. In order to do this, we consider a new data set wich contains for each sample a vector with the Hamming distance to each one of theother samples. \cite{deMingoLopez2009} This paper proposes a symbolic measure that is used to implement a string self-organizing map based on SOM algorithm.Such measure between two strings is a new string. Computation over strings is performed using a priority relationship among symbols.A complementary operation is defined in order to apply such measure to DNA strands. Finally, an algorithm is proposed in order to be ableto implement a string self organizing map. It is important to note that new symbols $a,b \in V \mid \mathcal{O}(a) - \mathcal{O}(b) \mid > 1$, and ther is no symbol $c$ such tah $\mathcal{O}(a) < \mathcal{O}(c) < \mathcal{O}(b)$. That is, \begin{displaymath}         \mathcal(O)^{-1}(k) = \left \{ \begin{array}{ll}         x \in V  if \mathcal{O}(x) = k & \textrm{}\\         S_k i.o.c. & \textrm{}\\                 \end{array} \right. \end{displaymath}$\mathcal(O)^{-1}(k)= \lbrace{ x \in V if \mathcal{O}(x) = k, sk i.o.c. with K \in N}$Self-Organizing Maps (SOM) (Tuevo Kohonen \cite{kohonen82a,kohonen95a}) are widely used for unsupervised learning, clustering, classification, and visualization of any kind of data. More than 5000 publications \cite{cottrell2006} can be found in the literature in the last years. Many of these works propose significant approaches to deal with vectorial data. The use of linear vector spaces in SOM decrease the complexity thanks to the Euclidean distance and to the arithmetical operations usually implemented as basic instructions in computer processors. These special characteristics of numerical vectors makes easy to implement fast SOM algorithms. However, areas with increasing relevance like image processing, bioinformatics, or speech recognition need to deal with nonvectorial data.There are two approaches to work in SOM with nonvectorial data. One approach consist on transforming input data to vectorial one. The other approach, is based on SOM algorithm modification to enable it to deal directly with a certain data type. Codify input data as numerical vectors, usually require high efforts for analysis and implementation. Sometimes, this transformation is not possible or reduce certain characteristics of original data. In a recent published work \cite{mora07} we made clustering and visualization of HIV Quasiespecies using SOM. In this paper we have codified original ARN sequences to vectors using \textit{Hamming} distance finding some inconveniences in this process. That experience gided as to work with the second approach.A good methodology to deal with nonvectorial data directly, is described by T. Kohonen and P. Somervuo in \cite{kohonen02a_bibuniq_416}. This method modify original SOM algorithm calculating a \textit{Median Set} as model. Studies over the median for symbol strings in \cite{higuera00} demonstrates the NP-Complete complexity. That inconvenience joint with the loss of graduality in learning process motivate us to search for a new methodology that will be nearer to the original SOM simplicity and elegance. In this way, we developed a method that start with SOM for vectorial data algorithm making minimum changes. We have been analized the algorithm in order to identify the steps that modify the models. In original SOM for vectors, the models has same type than input data. Once, we located these crucial steps, we modify it to make it more general. The idea is to provide the needed elements that permit us to work with any type of data. The \textit{model updating process} is one of these steps that "touch" directly the input data. Other operation like calculating neighborhood depends on data coordinates in the grid and not on data itself.In this paper we propose a framework for SOM that permit us to deal with any kind of data maintaining original vectorial SOM concepts and we formalizes the requirements to implement new modules for any input data type. As example we have implemented the AI::NeuralNet::SOMString Perl module. We present in Sec. \ref{examples} some results for strings clustering experiments.The paper is organized as follows: Sec. \ref{state} contains a brief survey of works related to the application of SOM to nonvectorial data. Sec. \ref{somdei} introduces our new algorithm based on the use of discrete entities interpolation. Sec. \ref{experiments} is devoted to describe two experemental examples in order to study how our proposal works with symbol strings and the perfomance of our method method with large data sets.%---------------------------------------------------------------------%  Self-organizing maps for nonvectorial data%---------------------------------------------------------------------\section{State of the art}Tengo que hablar de trabajos relacionados. Cosas que tengo que contar:- The batch map for strings: "Sammon" projection?The Self-Organizing Map (SOM) was introduced by Teuvo Kohonen in1982 (see \cite{kohonen82a,kohonen95a} for details).Learning Vector Quantization (LVQ) supervised learning. No neighborhood are defined around the winner. LVQ can be used to fine tuning the SOM reference vectors for best class separations \cite{kohonen98b}To deal with nonvectorial data, there are two approaches. One of them approaches is based on codify input data with real vectors. There are good examples of this method in \cite{honkela98a,RefMora,oja05a_bibuniq_36}. Other approach is based on create a map of models using directly original data. Changes in SOM algorithm are made to make it capable to deal directly with non vectorial data. One of these methods is \textit{Median SOM} described originally in \cite{kohonen96l} and Kohonen and Somervuo in \cite{kohonen02a_bibuniq_416} have shown how this algorithm works for clustering a large protein sequence database. We will describe it in section \ref{mediansom} describes this method. We find \textit{Median SOM} examples in \cite{oja05a_bibuniq_362}. We propose a new simple method that no changes on input data are needed. Will be possible to re-use early all the original implementation and need only to include two new functions \textit{integer distance(elementA, elementB)} and \textit{ element interpolateNewElement(distance, sourceElement, targetElement)}. Section \ref{somstring} describe this two new functions.An implementation of multidimensional vectorial SOM developed by T.Kohonen is available in \cite{kohonen96m}. After the seminal Kohonen works, several works have been published that try to enhance the original SOM implementation. Examples of these works are \cite{koikkalainen93a,lampinen90a}, and \cite{kohonen99f} where the authors use evolutionary-learning to make a faster SOM implementation without using a distance function.\section{Self-Organizing Map}\label{sec:SOM}The SOM is a non-supervised neural network that tries to imitate the self-organization done in the sensory cortex of the human brain, where neighbouring neurons are activated by similar stimulus. It is usually employed either as a clustering/classification tool or as a method to find unknownrelationships among a set of variables that describe a problem.The main property of the SOM is that it makes a nonlinear projectionfrom a high-dimensional data space (one dimension per variable) on aregular, low-dimensional (usually 2D) grid of neurons (see Figure\ref{fig:somgrid}), and, from the self-organization process, the projection preserves the topologic relations while simultaneously creating a dimensionalreduction of the representation space (the transformation is made ina topologically ordered way).% \begin{figure}[htpb]%\begin{center}%\includegraphics[width=12cm]{somgrid.eps}%\caption{SOM Grid structure. There is an input layer (with the samples) and a process layer (where the neurons of the network are) which takes a grid shape.%\label{fig:somgrid}}%\end{center}%\end{figure}%Since this type of network is distributed in a plane (2-dimensional%structure) it can be concluded that the projections preserve the%topologic relations while simultaneously creating a dimensional%reduction of the representation space (the transformation is made in%a topologically ordered way).The SOM processes a set of input vectors (samples or patterns),which are composed by variables (features) typifying each sample.It then creates an output topological network where each neuron isalso associated to a vector of variables (model vector) which isrepresentative of a group of the input vectors. Note in Figure\ref{fig:somgrid} that each neuron of the network is completelyconnected to all the nodes (each node is a sample) of the inputlayer. So, the network represents a feed-forward structure with onlyone computational layer formed by neurons or model vectors.There are four main steps in the processing of the SOM. Except thefirst one, the others are repeated until a stop criterion is met:\begin{itemize}\item \textbf{Initialization of model vectors}. Usually it is made by assigning small random values to their variables, but there are some other possibilities such as an initialization using random input samples.\item \textbf{Competitive process}. For each input pattern $X$, all the neurons (model vectors) $V$ compete using a \textit{similarity function} in order to identify the one most similar or closest to the sample vector. Usually, the similarity function is a distance measure (such as an Euclidean distance). The winner neuron is called the best matching unit (BMU).\item \textbf{Cooperative process}. The BMU determines the centre of a topological neighbourhood where those neurons inside it (the model vectors) will be updated to be even more similar to the input pattern. A \textit{neighbourhood function} is used to determine the neurons to consider. If the lattice where the neurons are is rectangular or hexagonal, it is possible to consider as neighbourhood functions rectangles or hexagons with the BMU as centre. However, it is more usual to use a Gaussian function to assure that the farther the neighbour neuron is, the smaller the updating to its associated vector is. In this process, all the neurons within a vicinity cooperate tolearn.\item \textbf{Learning process}. In this step the variables of the model vectors within the neighbourhood are updated to be closer to those of the input vector. It means making the neuron more similar to the sample. The \textit{learning rule} used to update the vector ($V$) for every neuron $i$ in the neighbourhood of the BMU is:{\small\begin{equation}\label{eq:learningrule}V_i^{t} = V_i^{t-1} + \alpha^{t} \cdot N_{BMU}^{t}(i) \cdot (X-V_i^{t-1})\end{equation}}Where $t$ is the current iteration of the whole process, $X$ is theinput vector, $N_{BMU}$ is the neighbourhood function for the BMU,which returns a high value (in [0,1]) if the neuron $i$ is in theneighbourhood and close to the BMU (1 if $i=BMU$), and a small valueotherwise (0 if $i$ is not located inside the neighbourhood); and$\alpha$ is the \textit{learning rate} (in (0,1]). Bothneighbourhood and learning rate depend on $t$, since it is usual todecrease the radius of the first one and the value of the second inorder to impose a higher updating rate at the beginning of theprocess and almost none in the final iterations.\end{itemize}%All this process is equivalent to 'move' the neurons in the grid, moving near similar ones and moving away those different. In addition, the neurons will beThe recurrent application of Equation \ref{eq:learningrule} and theupdate of the neighbourhood function, has the effect of `moving' themodel vectors, $V_j$ from the winning neuron towards the inputvector $X_i$. That is, the model vectors tend to follow thedistribution of the input vectors. Consequently, the algorithm leadsto a topological arrangement of the characteristic map of the inputspace, in the sense that adjacent neurons in the network tend tohave similar weights vectors.The SOM is further processed using Ultsch method \cite{UmatUlts}, theUnified distance matrix (U-Matrix). It uses SOM's codevectors(vectors of variables of the problem) as data source and generates amatrix where each component is a distance measure between twoadjacent neurons.The U-Matrix shows a lattice where there are one cell per neuron in the map, which is the best matching unit for one (or more) of the input samples, and one more cell between every pair of neurons, which represents the distance between them. This distance is showed using a color code (usually blue means near or little distance, and red far or large distance). Each cell corresponding to one neuron in the map may be labeled with the related tag of the pattern which it represents.It allows us to visualize any multi-variated dataset in a two-dimensional display, so we can detect topological relations among neurons and infer about the input data structure. High values in the U-matrix represent a frontier region between clusters, and low values represent a high degree of similaritiesamong neurons on that region, i.e. clusters.Therefore, looking at the output of a SOM and its corresponding U-Matrix, it is possible to recognize some clusters as well as the metric-topological relationsof the data items (vectors of variables of the problem) and theoutstanding variables.Although Kohonen's SOMs are not as accurate as other tools at thetask of classification, they can be applied to many different typesof data, yielding a visualization of natural structures in the dataand their relations, as well as the natural groupings that could beamong them. In addition, SOMs make easy the estimation of thevariables that have more influence on these groupings, via the so-called planes analysis.Other statistical and soft computing tools can also be used for thispurpose, but Kohonen's SOMs offer a visual way of doing it which ismuch more intuitive.%---------------------------------------------------------------------%  SOM algorithm with discrete entities interpolation%---------------------------------------------------------------------\section{A generalized SOM approach for any kind of data}\label{somdei}\subsection{\textbf{The SOM approach for vectorial data}}\begin{figure}% Use the relevant command to insert your figure file.% For example, with the graphic package use% \includegraphics[width=12cm]{som_update_process.eps}% figure caption is below the figure\caption{Left: SOM with 20 neurons (4 rows and 5 columns) and marked neighborhood area with radius 1, centered in winner neuron in a rectangular topology. Right: graphical representation of SOM training process}\label{figupdatingsom}       % Give a unique label\end{figure}SOM learning process iteratively perfoms the following three steps:\begin{itemize} \item A certain element of input data is chosen. \item The model of the SOM grid with the best matching with the selected datum is located \item This winner model and its neighborhood is appropriately updated\end{itemize} In this process, the distance measure plays a significant role:  the winner model is the \textit{closest} one to the input datum and the update is carried out with the aim of taking the neighborhood \textit{nearer} to the input datum (see Fig. \ref{figupdatingsom}). In the case of vectorial data, for each model $m_{i}$, the following formula is used to calculate the updated value:\begin{equation} m_{i}(t+1)= m_{i}(t)+h_{ci}(t)[x(t)-m_{i}(t)]\label{eqsomupdate}\end{equation}where $m_{i}(t)$ is the model located at $i$ grid position, $x(t)$ is the selected input datum, $ t=0,1,2... $ is the discrete-time coodinate, and $h_{ci}(t)$  is called \textit{neighborhood function}. This \textit{neighborhood function} usually takes a \textit{Gaussian} form:\begin{equation} h_{ci}(t)=\alpha(t) \cdot exp \left( - \frac{{\parallel r_{c}-r_{i}\parallel}^{2}}{2\sigma^{2}(t)}\right) \label{eqneighborhood}\end{equation}where $\alpha(t)$ is the learning factor in time $t$, $r_{c}$ and $r_{i}$ are the locations of models $c$ and $i$ respectively, and $\sigma(t)$ stands for the Gaussian width.\paragraph{}[include Gaussian function graphic]\paragraph{}The goal of this equation is to compute a new value $m_{i}(t+1)$ wich is $h_{ci}(t)[x(t)-m_{i}(t)]$ \textit{nearer} to $x(t)$ and $h_{ci}(t)[x(t)-m_{i}(t)]$ \textit{farer} from $m_{i}(t)$. With vectorial data the "+" operator together with the euclidean distance permits us to easily obtain the new value. However, if we are working with nonvectorial data this process has to be appropiately adapted. \subsection{\textbf{Working with any kind of data}}If we want to apply the above described process to nonvectorial data, we need to find a suitable distance measure and an operator that allows us to take the models closer to the selected input datum at each iteration. Let us formalize this idea.\begin{theorem}[Conditions for distance function] Let D be a data domain (vectorial or not). Let $d:D\times D\longrightarrow R$ be a distance function denoted d(x,y). The distance function must hold the following conditions foreach pairs of elements $x,y \in D$:\begin{itemize} \item[] C1) $d(x,y)\geq 0$ where equality holds if and only if $x=y$. \item[] C2) $d(x,y)=d(y,x)$ \item[] C3) $d(x,y)\leq d(x,z) + d(z,y)$ \end{itemize}\end{theorem}\begin{theorem}[Neighbor Data Set] Let D be a data domain.  Let $d:D\times D\longrightarrow R$ be a distance function with the above Theorem restrictions. The Neighbor Data Set of distance $\delta$ of value x, $N(x,\delta)$, is defined as follows:$$N(x,\delta)=\{y \in D|d(x,y)=\delta\}$$ \end{theorem}That is, the Neighbor Data Set contains all the elements of domain D that are at distance $\delta$ from a given datum x. With this definition, we can now proceed to define the operation that allows us to take a model closer to a given element of the domain.\begin{theorem}[Interpolation function] Let D be a data domain (vectorial or not).  let $d:D\times D\longrightarrow R$ be a distance function. We say that $I:D\times D\times R\longrightarrow D$ is an Interpolation function if the following property holds:$$I(x,y,\delta)\in N(x,\delta)\cap N(y,d(x,y)-\delta), \forall \delta\leq d(x,y)$$\end{theorem}That is, the interpolation function, given two elements x and y of the domain and a distance $\delta$, permits us to obtain a new element I(x,y,$\delta$) that is $\delta$ farer from x and $\delta$ closer to y.  In the case of our SOM grid, the interpolation function permits us to generalize equation \ref{eqsomupdate} as follows:\begin{equation} m_{i}(t+1)= I(m_{i}(t), x(t), h_{ci}(t)\cdot d(m_{i}(t),x(t))\label{eqsomupdatenew}\end{equation}This way, with the generalized versions of the + operator and the euclidean distance, we can now apply the above method to any kind of data with the only restriction of having both a suitable distance and interpolation functions.\subsection{\textbf{The extended algorithm}}\section{The case of strings}\subsection{\textbf{A distance measure for string data}}\paragraph{\textbf{Hamming Distance}}Originally this distance measure was defined for binary codes but can be applied to any ordered set of discrete values. The hamming distance is defined only for strings of equal length.The distance consist of the number of different symbols. \begin{verbatim}  x = (1,0,1,1,1)  y = (0,1,1,0,1)    dH(x,y)=3  u=(s,a,l,e)  v=(y,a,l,e)  dH(u,v)=1\end{verbatim}\paragraph{\textbf{Levenshtein or Edit distance}}The distance between strings A and B is defined as:\begin{equation}LD(A,B)= min\{a(i)+b(i)+c(i)\}\end{equation}B is obtained from A by a(i) replacements, b(i) insertions and c(i) deletions of a symbol. There exist an infinite number of combinations for $\{a(i)+b(i)+c(i)\}$, but the minimum can be formed using a dynamic-programming method. The distance measure is more reliable if the editing operations are provided with different weights. There are a weighted version of Levenshtein distance described by T. Okuda et al. in \cite{okuda1976}, with this equation:\begin{equation} WLD(A,B)= min\{pa(i)+qb(i)+rc(i)\}\end{equation}where \textit{p}, \textit{q} and \textit{r} may be obtained from the confusion-matrix of the alphabet, as the inverse probabilities for particular types of error to occur. However, this is only usefull for strings of words.\begin{figure}% Use the relevant command to insert your figure file.% For example, with the graphic package use% \includegraphics[width=6cm]{prueba_grafico.eps}% figure caption is below the figure\caption{Updating a SOM with symbol string models using interpolation: $x(t) = ABCD$ and $m_{i}(t)=A$. The new interpolated valued with, e.g., $h_{ci}(t)=1$ is $m_{i}(t+1)=AB$. Only one change that leave $m_{i}(t+1)$ nearest to $x(t)$. Left: before model updating, Right:After model updating.}\label{figupdatingsom}       % Give a unique label\end{figure}\paragraph{\textbf{String::Diff algorithm distance}}This distance measure is based on String::Diff Perl module developed by Kazuhiro Osawa. It is similar toLevenshtein distance, but without using replacements as a basic edit operation. The distance between strings A and B is defined as:\begin{equation}SDD(A,B)= min\{b(i)+c(i)\}\end{equation}B is obtained from A by b(i) insertions and c(i) deletions of a symbol\paragraph{}To compute string distance we use String::Diff Perl module. This is a fast algorithm to obtain strings differences. This is a execution example of String::Diff module wrote in Perl:\begin{verbatim}   my $diff = String::Diff::diff_merge("AB", "ABCDE",      remove_open => "<del>",      remove_close => "</del>",      append_open => "<ins>",      append_close => "</ins>",   );   print "$diff\n";# this is AB<ins>CDE</ins>\end{verbatim}Arguments \textit{remove\_open}, \textit{remove\_close}, \textit{append\_open} and \textit{append\_close} indicates the marks for insert or delete a substring. We have transformed original String::Diff output, to this one:\begin{verbatim}   #This is new diff string AB<ins>C</ins><ins>D</ins><ins>E</ins>\end{verbatim}It is easy to interpolate a new symbol string with a certain distance using this difference string. \subsection{\textbf{An interpolation function for string data}}\paragraph{\textbf{new\_interpolated\_model implementation for symbol strings}}Other important element required to compute new SOM algorithm for nonvectorial data is \textit{new\_interpolated\_model} function. We have to implement a different function for each data type we want to compute. In case of symbol strings we implemented two different version. One of this is based on Levenshtein algorithm. This version has complexity $O(n^{3})$. When string lengths is more than 1000 characters as will be the case of DNA strings, SOM algorithm time to compute increase drastically. This is the reason why we test different distances measures and finally decided to use String:Diff package. This implementation is fast for any string length. Other advantage this measure has got is the possibility to easily interpolate new string using difference string output. This is the pseudocode for $new\_interpolated\_model$ function:\paragraph{}    $new\_interpolated\_model(m_{i}(t), x(t), d(t))$ $\{$ \newline      \indent\indent$ diff = distance(m_{i}(t), x(t))$\newline      \indent\indent return $calculate\_new\_model(diff, d(t))$ \newline    $\}$ \newline where $calculate\_new\_model(diff, d(t))$ function, process the difference string returned by $distance(m_{i}(t), x(t))$ function doing $d(t)$ number of changes. Let the difference string returned by distance function:\begin{verbatim}   AB<ins>C</ins><ins>D</ins><ins>E</ins> \end{verbatim}   make $d(t)$ changes over it, will result the string:\begin{verbatim}    ABCD<ins>E</ins>\end{verbatim}to obtain the new interpolated string the only thing needed is to eliminate the substring starting at first \textit{"ins"} or \textit{"del"} and finishing at string end. In this example, new interpolated string will be "ABCD". The string have distance 2 from "AB" and is two positions nearer to "ABCDE"\paragraph{}\subsection{\textbf{The approach for string}}%---------------------------------------------------------------------%  Experiments%---------------------------------------------------------------------\section{Experimentation}\label{experiments}We included in this paper two experiments to illustrate how the algorithm works. First one, are developed using a short symbol string data set in order to easily check clustering capabilities. The second one, is made using a database with 100.000 english word to test how the algorithm works with a large database.\paragraph{}To test the new algorithm presented in Sec. \ref{somdei} we have implemented a new Perl module called AI::NeuralNet::SOMString. This Perl module is based on a previous implementation developed by Alexander Voischev  called of AI::NeuralNet::SOM. One element required to SOM implementation is a distance measure. There are several options to calculate a symbol string distance. We presents some of the most used symbol string distances.\paragraph{\textbf{Input selection}}An important aspect in SOM experiments is how input are ordered. If we have, e.g., an alphabetically ordered english dictionary as input data set, with radius around 3, we can see the whole map first with words starting with "A", then with words starting with "B" and same with all letters in alphabet. Depending on when we decided to stop the learning process, the map will show a clustering view or other drastically different. Will be better to merge randomly the dataset before use it.Table \ref{exp1arguments} represents SOM arguments for the experiment. Note that radius is nearest to SOM array dimension. Several experiments realized with smallest radius values, results in maps with no updated areas. Initial randomly used valued remain after training process. Gaussian Neighborhood function change deeply neurons that are closes to winner. Linear alpha function makes the transitions between clusters gradually. Fig. \ref{figexp1somtable} respresents SOM for strings using a initialization map with string randomly generated with a length no longer than three characters. Fig. \ref{figexp1simpleumatrix} represents umatrix for same SOM. Labels is umatrix is the result of map calibration. Taking each input value and locating in the map the nearest value. This map present several models with same distance to each input value. In this case we take one of them randomly.\subsection{\textbf{Simple data set experiment}}\label{expsimple}\paragraph{Data set description}This experiment uses same dataset than Panu Somervuo in \cite{somervuo04a_bibuniq_173} to easily compare results. Data sets consist on five strings: CAT, CATTLE, BAT, BATTLE and BATTLEFIELD.  \begin{table}% table caption is above the table\caption{Arguments for SOM using short symbol string dataset}\label{exp1arguments}       % Give a unique label% For LaTeX tables use\begin{tabular}{lll}\hline\noalign{\smallskip} \textbf{Argument}& \textbf{Value}\\\noalign{\smallskip}\hline\noalign{\smallskip}\textbf{Alpha} & 0.5\\\textbf{Radius} & 9.9\\\textbf{Neighborhood function} & Gaussian\\\textbf{Alpha function} & Linear\\\textbf{Rows} & 10\\\textbf{Columns} & 10\\\textbf{Rounds} & 27\\\noalign{\smallskip}\hline\end{tabular}\end{table}\begin{figure}% Use the relevant command to insert your figure file.% For example, with the graphic package use% \includegraphics[width=12cm]{som_table2.eps}% figure caption is below the figure\caption{Symbol string clustering 10x10 SOM after training process. Input data set consist of five strings: CAT, CATTLE, BAT, BATTLE and BATTLEFIELD. }\label{figexp1somtable}       % Give a unique label\end{figure}\begin{figure}% Use the relevant command to insert your figure file.% For example, with the graphic package use% \includegraphics[width=12cm]{umat_27_epoch_10x10_5_str_0.5_alpha_linear_alpha_type_9.9_radius_file_input_type_0.21_ver.ps}% figure caption is below the figure\caption{10x10 SOM umatrix representation. Input data set consist of five strings: CAT, CATTLE, BAT, BATTLE.  and BATTLEFIELD. Black represent zero distance. Dark gray represents low distance and light gray represents high distance. This map is calibrated with input values. Each label is located in one of the best matching node.}\label{figexp1simpleumatrix}       % Give a unique label\end{figure}\subsection{\textbf{Large database experiment}}\label{explarge}%---------------------------------------------------------------------%  Conclusions and further work%---------------------------------------------------------------------\section{Conclusions and further work}\label{conclusions}Mathematical properties of discrete learning process like convergence, etc.\begin{itemize} \item Developed methods for winner selection\end{itemize}Observed differences between continuous data input and discrete data input:\begin{enumerate} \item \textbf{Shorter training process} than training process for real vectors. The number of possible changes are limited by the maximum distance between SOM models and input elements.\item Depending on items distance and input data set diversity, after training, \textbf{SOM will present regions with zero distance}. This happends because possible values in training process are discrete and usually very short. In case of real values, after calculate a new \textit{model} $m_{i}(t+1)= m_{i}(t)+h_{ci}(t)[x(t)-m_{i}(t)]$ it is hard to have exactly the same values. Will be similar to use a discrete numerical space. Several input values will correspond to one output value, e.g., using a round function.\item \textbf{Winner selection}. In case of real data, is difficult to have exactly the same distance after apply adapt function. In case of discrete values with a short range, it is easy to have several winners. In these cases, selection method for winners will be used. We used two methods for winner selection. First one, is a randomly selected winner from winner set. Experiments realized with this \textit{winner selection method} for a large dataset cause no clustering, because each time a different area in map are selected for a certain model. [Sucesive] epochs will modify previous mini area and depending on radius will delete this mini area. Other method selection is to use first funded value. It is faster method and give good enough results. The reason why this method works is because first tends to be located always in same area and clustering is reinforced. When frontier values are selected to be updated, that updating process will not cause the deletion of a bigger cluster than in previous method. In a certain way, second one method (first winner) tends easily to clustering and first one (randomly winner selection) tends to dispersion.\item In case of real values, it is easy to see map areas where there are no input elements coincidences. Thanks to Euclidean distance, after train process \textit{models} are nearest to the areas where impact more input values. I case of non numerical input values, is most difficult to see that. We have not a Euclidean distance that give as a spacial distance measure for the elements in the map. A way to know how many input values impact in any area we use a graphic with. \end{enumerate}% \begin{acknowledgements}% If you'd like to thank anyone, place your comments here% and remove the percent signs.% \end{acknowledgements}%& BibTeX users please use one of\bibliographystyle{plain} % \bibliographystyle{spbasic}      % basic style, author-year citations%\bibliographystyle{spmpsci}      % mathematics and physical sciences%\bibliographystyle{spphys}       % APS-like style for physics%\bibliography{references}   % name your& BibTeX data base\bibliography{references}% Non-BibTeX users please use%\begin{thebibliography}{}%% and use \bibitem to create references. Consult the Instructions% for authors for reference list style.%% \bibitem{RefJ}% Format for Journal Reference% Author, Article title, Journal, Volume, page numbers (year)% Format for books% \bibitem{RefB}%& Author,& Book title, page numbers. Publisher, place (year)% % % etc%\end{thebibliography}\end{document}% end of file template.tex