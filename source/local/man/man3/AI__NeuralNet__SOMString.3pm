.\" Automatically generated by Pod::Man 2.25 (Pod::Simple 3.16)
.\"
.\" Standard preamble:
.\" ========================================================================
.de Sp \" Vertical space (when we can't use .PP)
.if t .sp .5v
.if n .sp
..
.de Vb \" Begin verbatim text
.ft CW
.nf
.ne \\$1
..
.de Ve \" End verbatim text
.ft R
.fi
..
.\" Set up some character translations and predefined strings.  \*(-- will
.\" give an unbreakable dash, \*(PI will give pi, \*(L" will give a left
.\" double quote, and \*(R" will give a right double quote.  \*(C+ will
.\" give a nicer C++.  Capital omega is used to do unbreakable dashes and
.\" therefore won't be available.  \*(C` and \*(C' expand to `' in nroff,
.\" nothing in troff, for use with C<>.
.tr \(*W-
.ds C+ C\v'-.1v'\h'-1p'\s-2+\h'-1p'+\s0\v'.1v'\h'-1p'
.ie n \{\
.    ds -- \(*W-
.    ds PI pi
.    if (\n(.H=4u)&(1m=24u) .ds -- \(*W\h'-12u'\(*W\h'-12u'-\" diablo 10 pitch
.    if (\n(.H=4u)&(1m=20u) .ds -- \(*W\h'-12u'\(*W\h'-8u'-\"  diablo 12 pitch
.    ds L" ""
.    ds R" ""
.    ds C` ""
.    ds C' ""
'br\}
.el\{\
.    ds -- \|\(em\|
.    ds PI \(*p
.    ds L" ``
.    ds R" ''
'br\}
.\"
.\" Escape single quotes in literal strings from groff's Unicode transform.
.ie \n(.g .ds Aq \(aq
.el       .ds Aq '
.\"
.\" If the F register is turned on, we'll generate index entries on stderr for
.\" titles (.TH), headers (.SH), subsections (.SS), items (.Ip), and index
.\" entries marked with X<> in POD.  Of course, you'll have to process the
.\" output yourself in some meaningful fashion.
.ie \nF \{\
.    de IX
.    tm Index:\\$1\t\\n%\t"\\$2"
..
.    nr % 0
.    rr F
.\}
.el \{\
.    de IX
..
.\}
.\"
.\" Accent mark definitions (@(#)ms.acc 1.5 88/02/08 SMI; from UCB 4.2).
.\" Fear.  Run.  Save yourself.  No user-serviceable parts.
.    \" fudge factors for nroff and troff
.if n \{\
.    ds #H 0
.    ds #V .8m
.    ds #F .3m
.    ds #[ \f1
.    ds #] \fP
.\}
.if t \{\
.    ds #H ((1u-(\\\\n(.fu%2u))*.13m)
.    ds #V .6m
.    ds #F 0
.    ds #[ \&
.    ds #] \&
.\}
.    \" simple accents for nroff and troff
.if n \{\
.    ds ' \&
.    ds ` \&
.    ds ^ \&
.    ds , \&
.    ds ~ ~
.    ds /
.\}
.if t \{\
.    ds ' \\k:\h'-(\\n(.wu*8/10-\*(#H)'\'\h"|\\n:u"
.    ds ` \\k:\h'-(\\n(.wu*8/10-\*(#H)'\`\h'|\\n:u'
.    ds ^ \\k:\h'-(\\n(.wu*10/11-\*(#H)'^\h'|\\n:u'
.    ds , \\k:\h'-(\\n(.wu*8/10)',\h'|\\n:u'
.    ds ~ \\k:\h'-(\\n(.wu-\*(#H-.1m)'~\h'|\\n:u'
.    ds / \\k:\h'-(\\n(.wu*8/10-\*(#H)'\z\(sl\h'|\\n:u'
.\}
.    \" troff and (daisy-wheel) nroff accents
.ds : \\k:\h'-(\\n(.wu*8/10-\*(#H+.1m+\*(#F)'\v'-\*(#V'\z.\h'.2m+\*(#F'.\h'|\\n:u'\v'\*(#V'
.ds 8 \h'\*(#H'\(*b\h'-\*(#H'
.ds o \\k:\h'-(\\n(.wu+\w'\(de'u-\*(#H)/2u'\v'-.3n'\*(#[\z\(de\v'.3n'\h'|\\n:u'\*(#]
.ds d- \h'\*(#H'\(pd\h'-\w'~'u'\v'-.25m'\f2\(hy\fP\v'.25m'\h'-\*(#H'
.ds D- D\\k:\h'-\w'D'u'\v'-.11m'\z\(hy\v'.11m'\h'|\\n:u'
.ds th \*(#[\v'.3m'\s+1I\s-1\v'-.3m'\h'-(\w'I'u*2/3)'\s-1o\s+1\*(#]
.ds Th \*(#[\s+2I\s-2\h'-\w'I'u*3/5'\v'-.3m'o\v'.3m'\*(#]
.ds ae a\h'-(\w'a'u*4/10)'e
.ds Ae A\h'-(\w'A'u*4/10)'E
.    \" corrections for vroff
.if v .ds ~ \\k:\h'-(\\n(.wu*9/10-\*(#H)'\s-2\u~\d\s+2\h'|\\n:u'
.if v .ds ^ \\k:\h'-(\\n(.wu*10/11-\*(#H)'\v'-.4m'^\v'.4m'\h'|\\n:u'
.    \" for low resolution devices (crt and lpr)
.if \n(.H>23 .if \n(.V>19 \
\{\
.    ds : e
.    ds 8 ss
.    ds o a
.    ds d- d\h'-1'\(ga
.    ds D- D\h'-1'\(hy
.    ds th \o'bp'
.    ds Th \o'LP'
.    ds ae ae
.    ds Ae AE
.\}
.rm #[ #] #H #V #F C
.\" ========================================================================
.\"
.IX Title "SOMString 3pm"
.TH SOMString 3pm "2008-06-20" "perl v5.14.2" "User Contributed Perl Documentation"
.\" For nroff, turn off justification.  Always turn off hyphenation; it makes
.\" way too many mistakes in technical documents.
.if n .ad l
.nh
.SH "NAME"
AI::NeuralNet::SOMString \- A simple Kohonen Self\-Organizing Maps with strings input data.
.SH "SYNOPSIS"
.IX Header "SYNOPSIS"
use AI::NeuralNet::SOMString;
.PP
.Vb 2
\&        # Create a new self\-organizing map.
\&        $som = AI::NeuralNet::SOMString\->new();
\&        
\&        # Create a data set to initialize and train.
\&        @data = (
\&        "a", "ab", "abc", "abcd", "abcde");
\&
\&        # Initialize map.
\&        $som\->initialize(3,3,5,\*(Aqhexa\*(Aq,\*(Aqbubble\*(Aq,\*(Aqrandom\*(Aq,0,\e@data);
\&        # only random initialization type is supported
\&
\&        # Find quantization error before training and print it.
\&        $qerr = $som\->qerror(\e@data);
\&        print "Mean quantization error before trainig= $qerr\en";
\&
\&        # Train map with the same data set.
\&        $som\->train(20,0.25,3,\*(Aqlinear\*(Aq,\e@data);
\&
\&        # Find quantization error after training and print it.
\&        $qerr = $som\->qerror(\e@data);
\&        print "Mean quantization error after trainig= $qerr\en\en";
\&
\&        # Create a data set to label map.
\&        @label_data = (
\&        23.508335, 21.359016, 3.906102, 4.884908, 404.440765,
\&        23.823174, 21.731325, 4.295785, 5.244288, 405.100342,
\&        24.207268, 22.070162, 4.646249, 5.030964, 404.812225,
\&        24.284208, 22.401424, 4.806539, 5.006081, 404.735596,
\&        24.401838, 22.588514, 4.957213, 5.011020, 404.176880,
\&        25.824610, 24.155489, 5.976608, 6.708979, 405.040466,
\&        26.197090, 24.353720, 6.272694, 6.843574, 405.728119,
\&        26.347252, 24.720333, 6.518201, 6.950599, 405.758606,
\&        26.537718, 24.976704, 6.661457, 7.163557, 404.037567,
\&        27.041384, 25.309855, 6.979992, 7.488787, 404.839081,
\&        27.193167, 25.601683, 7.173965, 7.920047, 404.749054);
\&
\&        #Label map with "fault" patterns.
\&        $patterns_count = scalar(@label_data) / $som\->i_dim;
\&        for $i (0..$patterns_count\-1){
\&                @pattern = splice(@label_data, 0, $som\->i_dim);
\&                ($x, $y) = $som\->winner(\e@pattern);
\&                $som\->set_label($x, $y, "fault");
\&        }
\&
\&        # Create a data set to test map.
\&        @test_data = (
\&        23.508335, 21.359016, 3.906102, 4.884908, X,
\&        23.823174, 21.731325, 4.295785, 5.244288, 405.100342,
\&        24.207268, 22.070162, 4.646249, 5.030964, 404.812225,
\&        13.575570, 12.656892, \-1.424328, \-2.302774, 404.921600,
\&        24.284208, 22.401424, 4.806539, 5.006081, 404.735596,
\&        24.401838, 22.588514, 4.957213, 5.011020, 404.176880,
\&        13.844373, 12.610620, \-1.435429, \-1.964423, 404.978180,
\&        24.628309, 23.015909, 5.075150, 5.560286, 403.773132,
\&        13.996934, 12.669785, \-1.384147, \-1.830788, 405.187378,
\&        25.551638, 23.864803, 5.774306, 6.208019, 403.946777,
\&        26.347252, 24.720333, 6.518201, 6.950599, 405.758606,
\&        26.537718, 24.976704, 6.661457, 7.163557, 404.037567,
\&        X, 15.601683, X, X, 404.749054,
\&        27.041384, 25.309855, 6.979992, 7.488787, 404.839081);
\&
\&        #Test map and print results.
\&        $patterns_count = scalar(@test_data) / $som\->i_dim;
\&        for $i (0..$patterns_count\-1){
\&                @pattern = splice(@test_data, 0, $som\->i_dim);
\&                ($x, $y) = $som\->winner(\e@pattern);
\&                $label=$som\->label($x, $y);
\&                if (defined($label)) {
\&                        print "@pattern \- $label\en";
\&                }
\&                else {
\&                        print "@pattern\en";
\&                }
\&        }
.Ve
.SH "DESCRIPTION"
.IX Header "DESCRIPTION"
.IP "The principle of the \s-1SOM\s0" 4
.IX Item "The principle of the SOM"
The Self-Organizing Map represents the result of a vector quantization
algorithm that places a number reference or of codebook vectors into
a high-dimensional input data space to approximate to its data sets
in an ordered fashion. When local-order relations are defined between
the reference vectors, the relative values of the latter are made to
depend on each other as if their neighboring values would lie along an
\&\*(L"elastic surface\*(R". By means of the self-organizing algorithm, this
\&\*(L"surface\*(R" becomes defined as a kind of nonlinear regression of the
reference vectors through the data points. A mapping from a
high-dimensional data space R^n onto, say, a two-dimensional lattice of
points is thereby also defined. Such a mapping can effectively be used
to visualize metric ordering relations of input samples. In
practice, the mapping is obtained as an asymptotic state in a learning
process.
.Sp
A typical application of this kind of \s-1SOM\s0 is in the analysis of
complex experimental vectorial data such as process states, where
the data elements may even be related to each other in a highly
nonlinear fashion.
.Sp
There exist many versions of the \s-1SOM\s0. The basic philosophy, however,
is very simple and already effective as such, and has been implemented
by the procedures contained in this package.
.Sp
The \s-1SOM\s0 here defines a mapping from the input data space R^n onto a
regular two-dimensional array of nodes.  With every node i, a
parametric reference vector mi in R^n is associated.  The lattice type of
the array can be defined as rectangular or hexagonal in this package;
the latter is more effective for visual display. An input vector x in
R^n is compared with the mi, and the best match is defined as
\&\*(L"winner\*(R": the input is thus mapped onto this location.
.Sp
One might say that the \s-1SOM\s0 is a \*(L"nonlinear projection\*(R" of the probability
density function of the high-dimensional input data onto the
two-dimensional display. Let x in R^n be an input data vector. It may be
compared with all the mi in any metric; in practical applications, the
smallest of the Euclidean distances ||x \- mi|| is usually made to define
the best-matching node, signified by the subscript c:
.Sp
\&\fB||x \- mc|| = min{||x \- mi||} ; or\fR
.Sp
\&\fBc  =   arg min{||x \- mi||} ; (1)\fR
.Sp
Thus x is mapped onto the node c relative to the parameter values mi.
.Sp
During learning, those nodes that are topographically close in the
array up to a certain distance will activate each other to learn from
the same input.  Without mathematical proof we state that useful
values of the mi can be found as convergence limits of the following
learning process, whereby the initial values of the \fImi\fR\|(0) can be
arbitrary, e.g., random:
.Sp
\&\fBmi(t + 1) = mi(t) + hci(t)[x(t) \- mi(t)] ; (2)\fR
.Sp
where t is an integer, the discrete-time coordinate, and hci(t) is
the so-called neighborhood kernel; it is a function defined over the
lattice points. Usually hci(t) = h(||rc \- ri||; t), where rc in R^2 and
ri in R^2 are the radius vectors of nodes c and i, respectively, in the
array. With increasing ||rc \- ri||, hci goes to 0. The average width and
form of hci defines the \*(L"stiffness\*(R" of the \*(L"elastic surface\*(R" to be
fitted to the data points. Notice that it is usually not desirable to
describe the exact form of p(x), especially if x is very-high-dimensional;
it is more important to be able to automatically find those dimensions and
domains in the signal space where x has significant amounts of sample values!
.Sp
This package contains two options for the definition of hci(t). The simpler
of them refers to a neighborhood set of array points around node c. Let this
index set be denoted Nc (notice that we can define Nc = Nc(t) as a function
of time), whereby hci = alpha(t) if i in Nc and hci = 0 if i not in Nc, where
alpha(t) is some monotonically decreasing function of time (0 < alpha(t) < 1).
This kind of kernel is nicknamed \*(L"bubble\*(R", because it relates to certain
activity \*(L"bubbles\*(R" in laterally connected neural networks [Kohonen 1989].
Another widely applied neighborhood kernel can be written in terms of the
Gaussian function,
.Sp
\&\fBhci = alpha(t) * exp(\-(||rc\-ri||^2)/(2 rad^2(t))); (3)\fR
.Sp
where  alpha(t) is another scalar-valued \*(L"learning rate\*(R", and the
parameter rad(t) defines the width of the kernel; the latter
corresponds to the radius of Nc above. Both alpha(t) and rad(t) are
some monotonically decreasing functions of time, and their exact forms
are not critical; they could thus be selected linear.  In this package
it is furher possible to use a function of the type alpha(t) = A/(B + t),
where A and B are constants; the inverse-time function is
justified theoretically, approximately at least, by the so-called
stochastic approximation theory.  It is advisable to use the inverse-time
type function with large maps and long training runs, to allow more
balanced finetuning of the reference vectors. Effective choices for
these functions and their parameters have so far only been
determined experimentally; such default definitions have been used in
this package.
.Sp
The next step is calibration of the map, in order to be able to locate
images of different input data items on it. In the practical
applications for which such maps are intended, it may be usually
self-evident from daily routines how a particular input data set ought
to be interpreted.  By inputting a number of typical, manually
analyzed data sets and looking where the best matches on the map
according to Eq.  (1) lie, the map or at least a subset of its nodes
can be labeled to delineate a \*(L"coordinate system\*(R" or at least a set of
characteristic reference points on it according to their manual
interpretation.  Since this mapping is assumed to be continuous along
some hypothetical \*(L"elastic surface\*(R", it may be self-evident how the
unknown data are interpreted by means of interpolation and
extrapolation with respect to these calibrated points.
.SS "\s-1METHODS\s0"
.IX Subsection "METHODS"
.IP "new AI::NeuralNet::SOMString;" 4
.IX Item "new AI::NeuralNet::SOMString;"
Creates a new empty Self-Organizing Map object;
.ie n .IP "$som\->initialize($xdim, $ydim, $idim, $topology, $neighborhood, $init_type, $random_seed, \e@data);" 4
.el .IP "\f(CW$som\fR\->initialize($xdim, \f(CW$ydim\fR, \f(CW$idim\fR, \f(CW$topology\fR, \f(CW$neighborhood\fR, \f(CW$init_type\fR, \f(CW$random_seed\fR, \e@data);" 4
.IX Item "$som->initialize($xdim, $ydim, $idim, $topology, $neighborhood, $init_type, $random_seed, @data);"
Initializes the \s-1SOM\s0 object. Sets map dimension \f(CW$xdim\fR x \f(CW$ydim\fR. Input data vector sets equal to \f(CW$idim\fR.
Variable \f(CW$topology\fR may be either \*(L"rect\*(R" or \*(L"hexa\*(R", \f(CW$neighborhood\fR may be \*(L"bubble\*(R" or \*(L"gaussian\*(R".
Initialization type of the \s-1SOM\s0 object can be \*(L"linear\*(R" or \*(L"random\*(R", \f(CW$random\fR seed is any non-negative 
integer. \e@data is a reference to the array containing initialization data.
.ie n .IP "$som\->train($train_length, $alpha, $radius, $alpha_type, \e@data);" 4
.el .IP "\f(CW$som\fR\->train($train_length, \f(CW$alpha\fR, \f(CW$radius\fR, \f(CW$alpha_type\fR, \e@data);" 4
.IX Item "$som->train($train_length, $alpha, $radius, $alpha_type, @data);"
The method trains the Self-Organizing Map.
\&\f(CW$train_length\fR \- a number of training epoches, \f(CW$alpha\fR \- learning rate, \f(CW$radius\fR \- initial training radius
which decreases to 1 during training process, \f(CW$alpha_type\fR sets a type of the learning rate decrease function, and can be \*(L"linear\*(R" or
\&\*(L"inverse_t\*(R", \e@data is a reference to the array containing training data.
.ie n .IP "$som\->qerror;" 4
.el .IP "\f(CW$som\fR\->qerror;" 4
.IX Item "$som->qerror;"
Returns quantization error of the trained map.
.ie n .IP "($x, $y, $dist) = $som\->winner(\e@data);" 4
.el .IP "($x, \f(CW$y\fR, \f(CW$dist\fR) = \f(CW$som\fR\->winner(\e@data);" 4
.IX Item "($x, $y, $dist) = $som->winner(@data);"
Finds the \*(L"winned\*(R" neuron for the mapped data vector \e@data and returns its coordinates \f(CW$x\fR and \f(CW$y\fR and \f(CW$dist\fR \- Euclidean 
distance between the neuron and the input vector.
.ie n .IP "$som\->set_label($x, $y, $label);" 4
.el .IP "\f(CW$som\fR\->set_label($x, \f(CW$y\fR, \f(CW$label\fR);" 4
.IX Item "$som->set_label($x, $y, $label);"
Sets label for the neuron with the coordinates x and y
.ie n .IP "$som\->clear_all_labels;" 4
.el .IP "\f(CW$som\fR\->clear_all_labels;" 4
.IX Item "$som->clear_all_labels;"
Clears all the labels on the map.
.ie n .IP "$som\->save(*FILE);" 4
.el .IP "\f(CW$som\fR\->save(*FILE);" 4
.IX Item "$som->save(*FILE);"
Save the Self-Organazing Map to file which represented as descriptor *FILE.
This may be *STDOUT.
The reference vectors are stored in ASCII-form. The format of the
entries is similar to that used in the input data files, except that
the optional iitems on the first line of data files (topology type, x\-
and y\-dimensions and neighborhood type) are now compulsory. In map
files it is possible to include several labels for each entry.
.Sp
An example: The map file code.cod contains a map of three-dimensional
vectors, with three times two map units.
.Sp
.Vb 1
\&      code.cod:
\&
\&       3 hexa 3 2 bubble
\&       191.105   199.014   21.6269
\&       215.389   156.693   63.8977
\&       242.999   111.141   106.704
\&       241.07    214.011   44.4638
\&       231.183   140.824   67.8754
\&       217.914   71.7228   90.2189
.Ve
.Sp
The x\-coordinates of the map (column numbers) may be thought to range
from 0 to n 1, where n is the x\-dimension of the map, and the
y\-coordinates (row numbers) from 0 to m 1, respectively, where m is
the y\-dimension of the map. The reference vectors of the map are
stored in the map file in the following order:
.Sp
.Vb 7
\& 1       The unit with coordinates (0; 0).
\& 2       The unit with coordinates (1; 0).
\&         ...
\& n       The unit with coordinates (n \- 1; 0).
\& n + 1   The unit with coordinates (0; 1).
\&         ...
\& nm      The last unit is the one with coordinates (n \- 1; m \- 1).
\&
\&
\&    (0,0) \- (1,0) \- (2,0) \- (3,0)         (0,0) \- (1,0) \- (2,0) \- (3,0)
\&
\&      |       |       |       |               \e    /   \e  /   \e   /   \e
\&
\&    (0,1) \- (1,1) \- (2,1) \- (3,1)             (0,1) \- (1,1) \- (2,1) \- (3,1)
\&
\&      |       |       |       |                 /   \e  /   \e   /   \e  /
\&
\&    (0,2) \- (1,2) \- (2,2) \- (3,2)         (0,2) \- (1,2) \- (2,2)  \-(3,2)
\&
\&
\&
\&          Rectangular                             Hexagonal
.Ve
.Sp
In the picture above the locations of the units in the two possible
topological structures are shown. The distance between two units in
the map is computed as an Euclidean distance in the (two dimensional)
map topology.
.ie n .IP "$som\->load(*FILE);" 4
.el .IP "\f(CW$som\fR\->load(*FILE);" 4
.IX Item "$som->load(*FILE);"
Loads the Self-Organazing Map from file which represented as descriptor *FILE.
.ie n .IP "$som\->umatrix;" 4
.el .IP "\f(CW$som\fR\->umatrix;" 4
.IX Item "$som->umatrix;"
Calculates Umatrix for existing map and returns a reference to array that contains Umatrix data.
.Sp
Umatrix is a way of representing the distances between reference vectors of neighboring map units.
Although being a somewhat laborious task to calculate it can effectively be used to visualize the
map in an interpretable manner.
.Sp
Umatrix algorithm calculates the distances between the neighboring neurons and stores them
in a grid (matrix) that corresponds to the used topology type. From that grid, a proper
visualization can be generated by picking the values for each neuron distance
(4 for rectangular and 6 for hexagonal topology). The distance values are scaled to the range
between 0 and 1 and are shown as colors when the Umatrix is visualized.
.Sp
Example:
.Sp
.Vb 9
\&        ...
\&        $umat = $som\->umatrix;
\&        for $j (0..$som\->y_dim*2\-2) {
\&                for $i (0..$som\->x_dim*2\-2) {
\&                        print "$umat\->[$j*($som\->x_dim*2\-1)+$i] ";
\&                }
\&                print "\en";
\&        }
\&        ...
.Ve
.ie n .IP "$som\->x_dim;" 4
.el .IP "\f(CW$som\fR\->x_dim;" 4
.IX Item "$som->x_dim;"
Returns the x dimention of map.
.ie n .IP "$som\->y_dim;" 4
.el .IP "\f(CW$som\fR\->y_dim;" 4
.IX Item "$som->y_dim;"
Returns the y dimension of map.
.ie n .IP "$som\->i_dim;" 4
.el .IP "\f(CW$som\fR\->i_dim;" 4
.IX Item "$som->i_dim;"
Returns the input vector dimension
.ie n .IP "$som\->topology;" 4
.el .IP "\f(CW$som\fR\->topology;" 4
.IX Item "$som->topology;"
Returns the map topology.
.ie n .IP "$som\->neighborhood;" 4
.el .IP "\f(CW$som\fR\->neighborhood;" 4
.IX Item "$som->neighborhood;"
Returns the neighborhood function type.
.ie n .IP "$som\->map($x, $y, $z);" 4
.el .IP "\f(CW$som\fR\->map($x, \f(CW$y\fR, \f(CW$z\fR);" 4
.IX Item "$som->map($x, $y, $z);"
Returns the \f(CW$z\fR element of the vector of the neuron with coordinates \f(CW$x\fR and \f(CW$y\fR. 
0 < \f(CW$z\fR <= \f(CW$som\fR\->i_dim.
.ie n .IP "$som\->label($x, $y);" 4
.el .IP "\f(CW$som\fR\->label($x, \f(CW$y\fR);" 4
.IX Item "$som->label($x, $y);"
Returns the label corresponding to the neuron with coordinates \f(CW$x\fR and \f(CW$y\fR.
.SH "NOTES"
.IX Header "NOTES"
.IP "Using missing values" 4
.IX Item "Using missing values"
You can use missing values in datasets to initialize and train map. I recommend to use \*(L"X\*(R" symbol
to indicate missing values, but you can use any alpha symbols for this purpose.
.Sp
Some particular parts of this documentation were taken from the documentation for \s-1SOM_PAK\s0
\&\fI<http://www.cis.hut.fi/research/som\-research/nnrc\-programs.shtml>\fR.
.SH "BUGS"
.IX Header "BUGS"
This is the alpha release of \f(CW\*(C`AI::NeuralNet::SOM\*(C'\fR, but I am sure 
there are probably bugs in here which I just have not found yet. If you find bugs in this module, I would 
appreciate it greatly if you could report them to me at \fI<voischev@mail.ru>\fR,
or, even better, try to patch them yourself and figure out why the bug is being buggy, and
send me the patched code, again at \fI<voischev@mail.ru>\fR.
.SH "HISTORY"
.IX Header "HISTORY"
AI\-NeuralNet\-SOM\-0.01 \- The first alpha version.
.PP
AI\-NeuralNet\-SOM\-0.02 \- fixed bugs in \*(L"load\*(R" method and added new method \*(L"umatrix\*(R".
.SH "AUTHOR"
.IX Header "AUTHOR"
Jose Carpio \fI<jose.carpio@dti.uhu.es>\fR adaptation for strings data input of  \f(CW\*(C`AI::NeuralNet::SOM\*(C'\fR
.PP
created by Voischev Alexander \fI<voischev@mail.ru>\fR
.PP
Copyright (c) 2008 Jose Carpio. All rights reserved. The \f(CW\*(C`AI::NeuralNet::SOMString\*(C'\fR are free software; 
you can redistribute it and/or modify it under the same terms as Perl itself.
\&\s-1THIS\s0 \s-1COME\s0 \s-1WITHOUT\s0 \s-1WARRANTY\s0 \s-1OF\s0 \s-1ANY\s0 \s-1KIND\s0.
